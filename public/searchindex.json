[{"section":"Blog","slug":"/blog/post-1/","title":"Using OpenTelemetry, OpenObserve, and Grafana to collect, transform, and visualize Logs, Metrics, and Traces","description":"","date":"March 21, 2025","image":null,"imageSM":null,"searchKeyword":"","categories":"Observability","tags":"AWS Lambda, HAProxy, Grafana, Kubernetes, Logs, Metrics, Minio, OpenObserve, OpenTelemetry, PostgreSQL, Traces, Traefik, Code:Terraform","content":"\n1. Introduction Since I’m hosting critical services—like a home automation platform, file storage, and photo storage—for people close to me, monitoring the health and performance of these systems is essential. With so many tools and services available today for logging, metrics, and tracing, it was quite a journey to select the individual components that eventually came together to form this observability stack. What began as a solution using the kube-prometheus-stack Helm chart and a wonky relay to forward AlertManager notifications to nfty.sh, eventually evolved into a more robust and scalable stack. The core of it consists of OpenTelemetry collectors, OpenObserve as the central monitoring hub, and Grafana to visualize and share the collected data.\n2. OpenTelemetry The OpenTelemetry Operator has been deployed with ArgoCD using its Helm chart. The Operator manages OpenTelemetry Collector instances, which are created via the opentelemetrycollectors.opentelemetry.io Custom Resource Definition (CRD). I maintain manifests for the collectors and apply them manually. All collectors use the otel/opentelemetry-collector-contrib image and are configured to use the same exporter for sending the collected data.\n1spec: 2 env: 3 - name: OPENOBSERVE_TOKEN 4 valueFrom: 5 secretKeyRef: 6 name: opentelemetry-o2token 7 key: o2token 8 config: 9 ... 10 exporters: 11 otlp/openobserve: 12 endpoint: openobserve-router.ns.svc.cluster.local:5081 13 headers: 14 Authorization: \u0026#34;Basic ${env:OPENOBSERVE_TOKEN}\u0026#34; 15 organization: default 16 tls: 17 insecure: true A separate OpenTelemetry collector is created for each receiver, so as to help spread the load of collecting data:\nhttpcheck: Collects health and performance metrics for HTTP endpoints. k8s_cluster: Collects cluster-level metrics and entity events from the Kubernetes API server. k8sobjects: Collects objects from the Kubernetes API server. k8slog: Collects pod logs across all nodes (in daemonset-stdout mode). kubeletstats: Collects node, pod, container, and volume metrics from the kubelet endpoint on each node. There isn’t much more to it to get started. I\u0026rsquo;ll create separate blog posts to provide more details on how the collectors are configured and used.\n3. OpenObserve OpenObserve serves as the central hub of my observability stack, offering a powerful, all-in-one solution for storing, querying, transforming and visualizing logs, metrics, and traces. While similar solutions offer feature parity with OpenObserve, it’s the ease of setup, simplicity in use, and fantastic out-of-the-box experience that truly sold me on it.\nAnother thing I appreciate about it is its scalability and performance. Its microservices-based architecture, with separate pods for the ingester, querier, router, alertmanager, NATS cluster, and compactor, allows each component to scale independently. Its ability to process and transform data using Pipelines and Functions is another big plus as it lets me filter, enrich, and reformat the incoming data before storing it.\nOpenObserve UI\n3.1 Environment Variables Here is the list of environment variables for which I changed the default values in the Helm chart values.yaml file:\n1ZO_CLUSTER_NAME: \u0026#34;o2-pluto\u0026#34; 2ZO_CLUSTER_COORDINATOR: \u0026#34;nats\u0026#34; 3ZO_COMPACT_DATA_RETENTION_DAYS: \u0026#34;30\u0026#34; 4ZO_LOCAL_MODE: \u0026#34;false\u0026#34; ## false indicates cluster mode deployment which supports multiple nodes with different roles. 5ZO_META_STORE: \u0026#34;postgres\u0026#34; 6ZO_NATS_ADDR: \u0026#34;nats://openobserve-nats.ns.svc.cluster.local:4222\u0026#34; 7ZO_S3_PROVIDER: \u0026#34;minio\u0026#34; 8ZO_S3_SERVER_URL: \u0026#34;http://a-hl.ns.svc.cluster.local:9000\u0026#34; 9ZO_S3_REGION_NAME: \u0026#34;eu-west-01\u0026#34; 10ZO_S3_BUCKET_NAME: \u0026#34;openobserve\u0026#34; 11ZO_SMTP_ENABLED: \u0026#34;true\u0026#34; 12ZO_SMTP_HOST: \u0026#34;in-v3.mailjet.com\u0026#34; 13ZO_SMTP_PORT: \u0026#34;587\u0026#34; 14ZO_SMTP_FROM_EMAIL: \u0026#34;admin_noreply@jlv6.com\u0026#34; 15ZO_SMTP_ENCRYPTION: \u0026#34;starttls\u0026#34; 16ZO_TELEMETRY: \u0026#34;false\u0026#34; 17ZO_WEB_URL: \u0026#34;https://\u0026lt;subdomain\u0026gt;.jlv6.com\u0026#34; The following keys exist in a Kubernetes secret named openobserve-secrets:\n1ZO_META_POSTGRES_DSN 2ZO_REPORT_USER_EMAIL 3ZO_REPORT_USER_PASSWORD 4ZO_ROOT_USER_EMAIL 5ZO_ROOT_USER_PASSWORD 6ZO_S3_ACCESS_KEY 7ZO_S3_SECRET_KEY 8ZO_SMTP_PASSWORD 9ZO_SMTP_USER_NAME 10ZO_TRACING_HEADER_KEY 11ZO_TRACING_HEADER_VALUE 3.2 RBAC \u0026amp; SSO limitations One of the more noticeable limitations with OpenObserve is that RBAC (Role-Based Access Control) and SSO (Single Sign-On) are enterprise-only features. While I understand that this is a common business model in the industry and that there needs to be a value-add for paying customers, it makes access management significantly more cumbersome in my setup. Because of this limitation:\nAll the OpenObserve user accounts I\u0026rsquo;ve created for authentication—including those for collectors, AWS services, and Grafana—have access to all data streams and other resources in the instance, as accounts can only be assigned the Admin role.\nThe Traefik Forward Auth middleware needs to be added to OpenObserve\u0026rsquo;s UI IngressRoute to fulfill the Multi-Factor Authentication (MFA) requirement I have for all the core services I\u0026rsquo;m hosting. As a result, users must authenticate twice: first by logging in via Authelia, which authenticates against an LLDAP backend, and then by authenticating again with a local OpenObserve user.\n4. Grafana The only reason Grafana is part of the stack is because OpenObserve does not (yet) provide a secure way to share its dashboards with anonymous viewers or embed them in an iframe. Accessing the Grafana UI can only be done as an authenticated user from a device with an RFC1918 IP address, additional Traefik ingressRoutes have been created to allow access to public-dashboards from anywhere.\n4.1 Configuration file (grafana.ini) 1[analytics] 2reporting_enabled = false 3check_for_updates = false ## Updates managed via its Helm chart. 4 5[date_formats] 6default_timezone = UTC 7 8[server] 9enable_gzip = true 10protocol = http 11http_addr = 0.0.0.0 12http_port = 3000 13root_url = https://\u0026lt;subdomain\u0026gt;.jlv6.com Grafana’s generic OAuth authentication method is used to enable Single Sign-On (SSO) for jlv6.com\u0026rsquo;s LDAP users through its integration with the Authelia OIDC Provider. Authelia authenticates users against the LLDAP backend and provides Grafana with relevant details, such as group membership, allowing Grafana to assign roles to LDAP users based on this information.\n1[auth.generic_oauth] 2enabled = true 3name = Authelia 4icon = signin 5auto_login = false 6client_id = grafana 7client_secret = \u0026lt;secret\u0026gt; 8scopes = openid profile email groups 9empty_scopes = false 10auth_url = https://.../api/oidc/authorization 11token_url = https://.../api/oidc/token 12api_url = https://.../api/oidc/userinfo 13login_attribute_path = preferred_username 14name_attribute_path = name 15groups_attribute_path = groups 16role_attribute_path = contains(groups, \u0026#39;GrafanaAdmins\u0026#39;) \u0026amp;\u0026amp; \u0026#39;Admin\u0026#39; || \u0026#39;Viewer\u0026#39; 17role_attribute_strict = true 18allow_assign_grafana_admin = false 19skip_org_role_sync = false 20use_pkce = true When using Grafana -\u0026gt; PgBouncer -\u0026gt; PostgreSQL, it’s important to set binary_parameters=yes in the connection string. Without this setting, Grafana may log errors like \u0026ldquo;unnamed prepared statement does not exist\u0026rdquo;.\n1[database] 2url = postgres://\u0026lt;db_user\u0026gt;:\u0026lt;db_user_password\u0026gt;@cloudnative-pg-black-pooler-rw.ns.svc.cluster.local:5432/\u0026lt;database\u0026gt;?sslmode=disable\u0026amp;binary_parameters=yes Set allow_embedding to true in the security section to allow embedding dashboards in an iframe.\n1[security] 2disable_initial_admin_creation = true 3allow_embedding = true I go into more detail about the plugin section in the next blog section: Querying OpenObserve logs.\n1[plugins] 2enable_alpha = true 3app_tls_skip_verify_insecure = false 4allow_loading_unsigned_plugins = zinclabs_openobserve 4.2 Querying OpenObserve logs The team behind OpenObserve has dedicated a documentation page to installing and configuring their Grafana plugin. Looking through its GitHub repository’s issues tab, there are some longstanding open issues, but none of them are affecting me. To set it up, I added an initContainer to the Grafana deployment that pulls the plugin and installs it at /var/lib/grafana/plugins.\nOnce the plugin was installed, I added it in the Grafana UI as a new data source… 4.3 Querying OpenObserve metrics Since OpenObserve exposes metrics by default, all I had to do was add a data source of type Prometheus in Grafana, point it at OpenObserve\u0026rsquo;s metric endpoint (http://openobserve-router.ns.svc.cluster.default:5080/api/default/prometheus), and configure basic authentication. 5. Why not use these alternatives? As is often the case, there’s more than one way to skin a cat. Below, you’ll find a list of alternatives I’ve considered at various points during this process.\n5.1 OpenTelemetry-eBPF I wish I could have used OpenTelemetry-eBPF\u0026rsquo;s kernel collector to gather low-level telemetry directly from the Linux kernel using eBPF. Unfortunately, the project doesn’t provide ARM64 images for its collectors, preventing me from deploying it across all nodes in my primary Kubernetes cluster. That said, the project is defintely worth checking out, as it also includes a Kubernetes and cloud collector, which enhance the collected telemetry with workload-specific metadata.\n5.2 AWS CloudWatch Although CloudWatch supports all the collectors I’m running, the costs associated with ingesting and storing logs, metrics, and traces from non-AWS services—along with the expenses for custom dashboards—make it too cost-prohibitive to be the central hub for monitoring activities.\n5.3 AWS CloudFront standard access or real-time logs While I collect standard access logs for all CloudFront distributions in a centralized S3 bucket, I rarely use them. They’re nice to have for occasional queries, but from a monitoring perspective, I’m primarily interested in real-time logs.\nWhich brings us to the option of streaming real-time logs from CloudFront to OpenObserve using AWS Kinesis Firehose. It is the approach I’d recommend, as it provides all the essential data and is relatively easy to set up. Unfortunately, the cost of running even the smallest Firehose configuration (mode=provisioned, shard=1, retention_period=24hr) made it too cost-prohibitive for a hobby project. Even when limiting it to this site’s production distribution—with minimal traffic—I was accumulating an average cost of $5/day. For comparison, my total monthly AWS bill is less than $0.30.\nIf you’re interested in implementing this approach, I highly recommend checking out this blog post. These are the code snippets I\u0026rsquo;ve used to enable real-time logging according to the post, with Terraform\u0026hellip;\nCode snippets 1resource \u0026#34;aws_kinesis_stream\u0026#34; \u0026#34;kinesis_stream_xx\u0026#34; { 2 provider = aws.us_east_1 3 name = \u0026#34;cf-logs-jlv6-production\u0026#34; 4 shard_count = 1 5 retention_period = 24 6 7 stream_mode_details { 8 stream_mode = \u0026#34;PROVISIONED\u0026#34; 9 } 10} 11 12resource \u0026#34;aws_kinesis_firehose_delivery_stream\u0026#34; \u0026#34;kinesis_firehose_stream_xx\u0026#34; { 13 provider = aws.us_east_1 14 15 name = \u0026#34;cf-logs-jlv6-production\u0026#34; 16 destination = \u0026#34;http_endpoint\u0026#34; 17 18 kinesis_source_configuration { 19 role_arn = aws_iam_role.iam_role_xx.arn 20 kinesis_stream_arn = aws_kinesis_stream.kinesis_stream_xx.arn 21 } 22 23 http_endpoint_configuration { 24 url = \u0026#34;https://openobserve.jlv6.com/aws/default/cloudwatch_metrics/_kinesis_firehose\u0026#34; 25 name = \u0026#34;OpenObserve instance on Pluto\u0026#34; 26 access_key = \u0026#34;\u0026lt;ACCESS_KEY\u0026gt;\u0026#34; 27 buffering_size = 1 28 buffering_interval = 60 29 role_arn = aws_iam_role.iam_role_xx.arn 30 s3_backup_mode = \u0026#34;FailedDataOnly\u0026#34; 31 32 s3_configuration { 33 role_arn = aws_iam_role.iam_role_xx.arn 34 bucket_arn = module.s3_bucket_xx.s3_bucket_arn 35 buffering_size = 5 36 buffering_interval = 300 37 compression_format = \u0026#34;GZIP\u0026#34; 38 } 39 40 processing_configuration { 41 enabled = \u0026#34;true\u0026#34; 42 processors { 43 type = \u0026#34;Lambda\u0026#34; 44 parameters { 45 parameter_name = \u0026#34;LambdaArn\u0026#34; 46 parameter_value = \u0026#34;${module.lambda_xx.lambda_function_qualified_arn}\u0026#34; 47 } 48 parameters { 49 parameter_name = \u0026#34;BufferSizeInMBs\u0026#34; 50 parameter_value = \u0026#34;3\u0026#34; 51 } 52 parameters { 53 parameter_name = \u0026#34;BufferIntervalInSeconds\u0026#34; 54 parameter_value = \u0026#34;45\u0026#34; 55 } 56 } 57 } 58 request_configuration { 59 content_encoding = \u0026#34;NONE\u0026#34; 60 } 61 } 62} 63 64resource \u0026#34;aws_cloudfront_realtime_log_config\u0026#34; \u0026#34;cf_realtime_log_config_xx\u0026#34; { 65 provider = aws.us_east_1 66 depends_on = [ 67 aws_iam_role.iam_role_xx, 68 aws_kinesis_stream.kinesis_stream_xx 69 ] 70 71 name = \u0026#34;jlv6-www\u0026#34; 72 sampling_rate = 100 73 74 fields = [ 75 \u0026#34;timestamp\u0026#34;, 76 \u0026#34;c-ip\u0026#34;, 77 \u0026#34;cs-method\u0026#34;, 78 \u0026#34;cs-uri-stem\u0026#34;, 79 \u0026#34;sc-status\u0026#34;, 80 \u0026#34;x-edge-result-type\u0026#34;, 81 \u0026#34;x-edge-response-result-type\u0026#34;, 82 \u0026#34;cs-user-agent\u0026#34;, 83 \u0026#34;cs-referer\u0026#34; 84 ] 85 86 endpoint { 87 stream_type = \u0026#34;Kinesis\u0026#34; 88 89 kinesis_stream_config { 90 role_arn = aws_iam_role.iam_role_xx.arn 91 stream_arn = aws_kinesis_stream.kinesis_stream_xx.arn 92 } 93 } 94} 95 96module \u0026#34;cf_distribution_xx\u0026#34; { 97 source = \u0026#34;terraform-aws-modules/cloudfront/aws\u0026#34; 98 providers = { 99 aws = aws.us_east_1 100 } 101 102 aliases = [\u0026#34;www.jlv6.com\u0026#34;] 103 104 comment = \u0026#34;Production distribution for www.jlv6.com\u0026#34; 105 enabled = true 106 is_ipv6_enabled = false 107 price_class = \u0026#34;PriceClass_100\u0026#34; 108 retain_on_delete = false 109 wait_for_deployment = true 110 111 geo_restriction = { 112 restriction_type = \u0026#34;whitelist\u0026#34; 113 locations = [\u0026#34;US\u0026#34;, \u0026#34;CA\u0026#34;, \u0026#34;GB\u0026#34;, \u0026#34;DE\u0026#34;, \u0026#34;FR\u0026#34;, \u0026#34;BE\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;LU\u0026#34;] 114 } 115 116 create_origin_access_identity = false 117 create_origin_access_control = false 118 119 logging_config = { 120 bucket = data.terraform_remote_state.shared.outputs.module_s3_bucket_xx_s3_bucket_bucket_domain_name 121 prefix = \u0026#34;cf_production\u0026#34; 122 } 123 124 origin = { 125 primaryK8S = { 126 domain_name = \u0026lt;subdomain\u0026gt;.jlv6.com 127 custom_origin_config = { 128 http_port = 80 129 https_port = 443 130 origin_protocol_policy = \u0026#34;https-only\u0026#34; 131 origin_ssl_protocols = [\u0026#34;TLSv1.2\u0026#34;] 132 } 133 } 134 } 135 136 default_cache_behavior = { 137 target_origin_id = \u0026#34;primaryK8S\u0026#34; 138 viewer_protocol_policy = \u0026#34;https-only\u0026#34; 139 allowed_methods = [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;] 140 cached_methods = [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;] 141 compress = true 142 143 use_forwarded_values = false 144 cache_policy_id = \u0026#34;658327ea-f89d-4fab-a63d-7e88639e58f6\u0026#34; # Managed-CachingOptimized 145 origin_request_policy_id = \u0026#34;33f36d7e-f396-46d9-90e0-52428a34d9dc\u0026#34; # Managed-AllViewerAndCloudFrontHeaders-2022-06 146 response_headers_policy_id = \u0026#34;67f7725c-6f97-4210-82d7-5512b31e9d03\u0026#34; # Managed-SecurityHeadersPolicy 147 148 realtime_log_config_arn = aws_cloudfront_realtime_log_config.cf_realtime_log_config_xx.arn 149 150 lambda_function_association = { 151 viewer-request = { 152 include_body = false 153 lambda_arn = data.terraform_remote_state.shared.outputs.module_lambda_at_edge_xx_lambda_function_qualified_arn 154 } 155 } 156 } 157 158 viewer_certificate = { 159 cloudfront_default_certificate = false 160 acm_certificate_arn = data.terraform_remote_state.shared.outputs.aws_acm_certificate_certxx_arn 161 ssl_support_method = \u0026#34;sni-only\u0026#34; 162 minimum_protocol_version = \u0026#34;TLSv1.2_2021\u0026#34; 163 } 164} 5.4 Grafana Cloud (free tier) While the free tier is quite generous, I quickly hit its 10K Metrics limit after choosing Grafana Cloud as the aggregator for all logs, metrics, and traces. I considered keeping it as part of the stack solely to share dashboards publicly, but the free hosted offering doesn’t seem to support that option. As a result, I opted to run an instance of Grafana OSS on Kubernetes, which allows me to set allow_embedding = true in its configuration file.\nThis concludes the blog post. If you have any questions, feel free to reach out at admin@jlv6.com.\n"}]